{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "def remove_pattern(row_content):\n",
    "    return re.sub(r'[?|$|.|!]',r'', row_content)\n",
    "def tokenize(row_content):\n",
    "    return ViTokenizer.tokenize(row_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID  Sentiment                                            Content\n",
      "0      0          1  Khung_cảnh đặc_trưng chưa bị pha_tạp Có nhiều ...\n",
      "1      1          1  Tôi có dịp vào Gia_lai công_tác , sau ngày làm...\n",
      "2      2          1  Biển đẹp , buổi tối ở bãi biển rất tấp_nập , q...\n",
      "3      3          0           hải_sản ko phong_phú và chế_biến ko ngon\n",
      "4      4          1  Tôi đến Hội_An khoảng 20 lần , lần nào cũng và...\n",
      "5      5          1  Chùa trên núi cao , rất mát_mẻ Chùa quả là hoà...\n",
      "6      6          1  Tôi có đến Dinh_III , ấn_tượng với các hiện_vậ...\n",
      "7      7          1  Nếu mục_đích của bạn đến Bangkok là để mua_sắm...\n",
      "8      9          1  Đây là 1 khách_sạn chứ không phải resort nhưng...\n",
      "9     10          1  Nhà_thờ nằm ngay khu_vực trung_tâm , nhưng có ...\n",
      "10    11          1  Chùa rộng , nằm trên đồi cao , tách_biệt khu d...\n",
      "11    12          0  Bãi biển Cha_Am không phẳng , bãi cát không mị...\n",
      "12    13          0  Ngay gần Rừng_Quốc_gia Ba Vì , không_gian rất ...\n",
      "13    14          1  Bảo_tàng được bài_trí công_phu , chia thành cá...\n",
      "14    15          1  Mình đã qua chợ mua một_số quà cho bạn_bè trướ...\n",
      "15    16          1  Đầu năm tôi có đi Yên_tử , lúc xuống có vào Th...\n",
      "16    17          1  Đầu xuân những năm gần đây nhà tôi hay đi chơi...\n",
      "17    18          0  Tôi đã đến thánh_địa Mỹ_sơn , lúc đầu tôi hình...\n",
      "18    19          1  Rất nên đi , cảm_giác đi thuyền trên sông , và...\n",
      "19    20          1  Làng nằm cách trung_tâm Hà_Nội chưa tới 50km ,...\n",
      "20    21          0  Không_gian chùa khá hẹp , chắc do nằm ngay khu...\n",
      "21    22          1  Nhà_ga rất nhỏ nhưng cũng là duyên_dáng nhất m...\n",
      "22    23          0  Chợ đêm này bình_thường , không có hoạt_động h...\n",
      "23    24          1  Ga xe lửa nhỏ xinh , đẹp như trong truyện_cổ_t...\n",
      "24    25          1  Không nên bỏ lỡ dịp tới thăm chợ đêm Cicada Ch...\n",
      "25    26          0  Trừ dịp có sự_kiện thì cũng hay , còn ngày thư...\n",
      "26    27          1  Chùa rất cổ , chùa nằm trên đỉnh đồi , lên chù...\n",
      "27    28          0  Theo mình nghĩ thì mọi người chủ_yếu đến đây đ...\n",
      "28    29          0  Hôm tự thuê xe_tuk - tuk đi chơi Angkor , buổi...\n",
      "29    30          1  Ở Lotte này tôi mới trải nghiệm Sky walk vào m...\n",
      "..   ...        ...                                                ...\n",
      "120  124          1  Nhà_thờ đá nằm ở ngay trung_tâm Sapa , tiện ch...\n",
      "121  125          1  Phải nói thật là đi cả Cô Tô và Thanh_Lân sẽ d...\n",
      "122  126          0  chỉ đi trên thuyền là cảm_thấy thư_thái , còn ...\n",
      "123  127          0  Đảo Cô Tô rất đông người đến du_lịch cho_nên m...\n",
      "124  128          1  Nước biển rất mát , trong như gương Đặc_biệt d...\n",
      "125  129          1  Bãi biển sạch đẹp cho cả việc chụp ảnh và tắm ...\n",
      "126  130          1  Resort gần biển , có view nhìn ra biển , vị_tr...\n",
      "127  131          1  Cảnh đẹp , nếu bạn và gia_đình đi nghỉ thì 2 n...\n",
      "128  132          1  Phía trong thành rất yên_tĩnh , nhiều cây_cối ...\n",
      "129  133          0  Nghe bạn giới_thiệu lên Sapa phải thử , nhưng ...\n",
      "130  134          1  Biển tương_đối êm , nước trong , dễ_dàng vui_c...\n",
      "131  135          1  Nhiều cảm_xúc ùa về khi tới tham_quan , khó có...\n",
      "132  136          1  Chợ đêm bán rất nhiều thứ Mà_ăn lại ngon nữa N...\n",
      "133  137          0  Ở đây đi chụp ảnh thì được chứ nếu để đi du_lị...\n",
      "134  138          1  Khách_sạn có 26 phòng tiện_nghi sang_trọng nằm...\n",
      "135  139          1  Bạn nên đến đây khi Bình_Ba còn chưa bị thương...\n",
      "136  140          1  Cũng giống như bao bãi biển khác như Sầm_Sơn ,...\n",
      "137  141          1  Resort chỉ đạt mức 3 sao , có_thể do được xây_...\n",
      "138  142          1  Vào trong thì rất mát Ở trong rất đẹp Rất nhiề...\n",
      "139  143          1  Khi bước vào bảo_tàng cảm_giác sẽ ngỡ_ngàng vì...\n",
      "140  144          1  Chùa rất cổ_kính , đẹp , yên_tĩnh Đi từ Hà_Nội...\n",
      "141  145          1  Vincom_Bà_Triệu gồm các phân khu chức_năng rộn...\n",
      "142  146          1  Người Hà_Nội đã quen_thuộc với hoạt_động chợ đ...\n",
      "143  147          1  Đến Bangkok mà không ghé thăm khu chợ_trời Cha...\n",
      "144  148          1  Đây có_thể là đoạn đường đèo trải nhựa dài nhấ...\n",
      "145  149          0  Hôm trước đưa cả công_ty đi Chùa_Hương và ghé ...\n",
      "146  150          1  Về với Nha_Trang , bạn cần mùa những sản_phẩm ...\n",
      "147  151          1  Nhà_mình đã đến tham_quan vườn thú mở Khao_Khe...\n",
      "148  152          1  Chợ có đủ loại mặt_hàng với giá_cả vừa túi_tiề...\n",
      "149  153          1  Hay nhất trong khu này là có vườn sim , nếu đi...\n",
      "\n",
      "[150 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import re\n",
    "df = pandas.read_csv('train.txt', sep='\\t')\n",
    "# print(df)\n",
    "df[\"Content\"] = df[\"Content\"].apply(remove_pattern)\n",
    "df[\"Content\"]= df[\"Content\"].apply(tokenize)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(content):\n",
    "    count_vect = CountVectorizer()\n",
    "    X_counts = count_vect.fit_transform(content)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "    return X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51, 1638)\n",
      "(150, 1852)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "\n",
    "import pandas\n",
    "import re\n",
    "test = pandas.read_csv('test.txt', sep='\\t')\n",
    "# print(df)\n",
    "test[\"Content\"] = test[\"Content\"].apply(remove_pattern)\n",
    "test[\"Content\"]= test[\"Content\"].apply(tokenize)\n",
    "# print(test)\n",
    "X_test_tfidf = preprocessing(test['Content'])\n",
    "print(X_test_tfidf.shape)\n",
    "X_train_tfidf = preprocessing(df['Content'])\n",
    "print(X_train_tfidf.shape)\n",
    "clf = GaussianNB().fit(X_train_tfidf.toarray(), df['Sentiment'])\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (51,1638) (1852,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-16788f1452b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# result = loaded_model.score(X_test_tfidf.toarray(), test['Sentiment'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mjll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/jupyter/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_joint_log_likelihood\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mjointi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_prior_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mn_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             n_ij -= 0.5 * np.sum(((X - self.theta_[i, :]) ** 2) /\n\u001b[0m\u001b[1;32m    438\u001b[0m                                  (self.sigma_[i, :]), 1)\n\u001b[1;32m    439\u001b[0m             \u001b[0mjoint_log_likelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjointi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_ij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (51,1638) (1852,) "
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "print(test['Sentiment'].shape)\n",
    "# result = loaded_model.score(X_test_tfidf.toarray(), test['Sentiment'])\n",
    "# print(result)\n",
    "y_pred = clf.predict(X_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trường đại_học bách_khoa hà_nội\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "average_precision = average_precision_score(test['Sentiment'], y_score)\n",
    "print(f1_score(test['Sentiment'], y_pred, average=\"macro\"))\n",
    "    print(precision_score(test['Sentiment'], y_pred, average=\"macro\"))\n",
    "    print(recall_score(test['Sentiment'], y_pred, average=\"macro\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
