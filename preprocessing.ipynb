{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvi import ViTokenizer, ViPosTagger\n",
    "def remove_pattern(row_content):\n",
    "    return re.sub(r'[?|$|.|!]',r'', row_content)\n",
    "def tokenize(row_content):\n",
    "    return ViTokenizer.tokenize(row_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ID Sentiment                                            Content\n",
      "0      0         1  Khung_cảnh đặc_trưng chưa bị pha_tạp Có nhiều ...\n",
      "1      1         1  Tôi có dịp vào Gia_lai công_tác , sau ngày làm...\n",
      "2      2         1  Biển đẹp , buổi tối ở bãi biển rất tấp_nập , q...\n",
      "3      3         0           hải_sản ko phong_phú và chế_biến ko ngon\n",
      "4      4         1  Tôi đến Hội_An khoảng 20 lần , lần nào cũng và...\n",
      "5      5         1  Chùa trên núi cao , rất mát_mẻ Chùa quả là hoà...\n",
      "6      6         1  Tôi có đến Dinh_III , ấn_tượng với các hiện_vậ...\n",
      "7      7         1  Nếu mục_đích của bạn đến Bangkok là để mua_sắm...\n",
      "8      9         1  Đây là 1 khách_sạn chứ không phải resort nhưng...\n",
      "9     10         1  Nhà_thờ nằm ngay khu_vực trung_tâm , nhưng có ...\n",
      "10    11         1  Chùa rộng , nằm trên đồi cao , tách_biệt khu d...\n",
      "11    12         0  Bãi biển Cha_Am không phẳng , bãi cát không mị...\n",
      "12    13         0  Ngay gần Rừng_Quốc_gia Ba Vì , không_gian rất ...\n",
      "13    14         1  Bảo_tàng được bài_trí công_phu , chia thành cá...\n",
      "14    15         1  Mình đã qua chợ mua một_số quà cho bạn_bè trướ...\n",
      "15    16         1  Đầu năm tôi có đi Yên_tử , lúc xuống có vào Th...\n",
      "16    17         1  Đầu xuân những năm gần đây nhà tôi hay đi chơi...\n",
      "17    18         0  Tôi đã đến thánh_địa Mỹ_sơn , lúc đầu tôi hình...\n",
      "18    19         1  Rất nên đi , cảm_giác đi thuyền trên sông , và...\n",
      "19    20         1  Làng nằm cách trung_tâm Hà_Nội chưa tới 50km ,...\n",
      "20    21         0  Không_gian chùa khá hẹp , chắc do nằm ngay khu...\n",
      "21    22         1  Nhà_ga rất nhỏ nhưng cũng là duyên_dáng nhất m...\n",
      "22    23         0  Chợ đêm này bình_thường , không có hoạt_động h...\n",
      "23    24         1  Ga xe lửa nhỏ xinh , đẹp như trong truyện_cổ_t...\n",
      "24    25         1  Không nên bỏ lỡ dịp tới thăm chợ đêm Cicada Ch...\n",
      "25    26         0  Trừ dịp có sự_kiện thì cũng hay , còn ngày thư...\n",
      "26    27         1  Chùa rất cổ , chùa nằm trên đỉnh đồi , lên chù...\n",
      "27    28         0  Theo mình nghĩ thì mọi người chủ_yếu đến đây đ...\n",
      "28    29         0  Hôm tự thuê xe_tuk - tuk đi chơi Angkor , buổi...\n",
      "29    30         1  Ở Lotte này tôi mới trải nghiệm Sky walk vào m...\n",
      "..   ...       ...                                                ...\n",
      "172  175         1  Là 3 điểm highlights của Nghệ_An Được_cái gần ...\n",
      "173  176         1  Phong_cảnh ở Bali thật tuyệt_vời , người dân m...\n",
      "174  177         1  Bãi biển đẹp và sạch hơn rất nhiều so với Patt...\n",
      "175  178         1  Angkok_Wat , Angkok Thom thật là hùng_vĩ , xứn...\n",
      "176  179         1  Chỉ có_thể nói rằng đây là nơi lý_tưởng và đán...\n",
      "177  180         1  Các gánh cơm hến , chè , bánh đặc_sản của Huế ...\n",
      "178  181         1  Mình chọn Sapa_Lake View Hotel cho chuyến du_l...\n",
      "179  182         1  Dãy phòng Garden_View tại khu nhà cấp 4 giống ...\n",
      "180  183         1  Gần 1 ngày được trải nghiệm cảm_giác làm một n...\n",
      "181  184         1  Chùa thì không đẹp , phải đi vòng_vèo chút qua...\n",
      "182  185         1  Thiên_nhiên ưu_đãi , khí_hậu mát_mẻ quanh_năm ...\n",
      "183  186         1  Khu này rất rộng , hình_như có đủ cả 54 kiểu n...\n",
      "184  187         0  Phòng khá sạch_sẽ , nhưng không_thể chịu nổi_n...\n",
      "185  188         1  Thực_sự yên bình và lãng_mạn Phố cổ hoành_trán...\n",
      "186  189         1  Tôi không có gì phàn_nàn về kỳ nghỉ_dưỡng của ...\n",
      "187  190         1  Khách_sạn đẹp , gần sân_bay , giá rẽ , nhiều đ...\n",
      "188  191         1  Chuyến đi chỉ có 2 người đến thành_phố Hoa Đà_...\n",
      "189  192         0  Khách_sạn này tên là khách_sạn buồn Phòng_ốc ,...\n",
      "190  193         0  Đường_không quá dốc và vất_vả , theo lời kể củ...\n",
      "191  194         0  Nói thế_nào nhỉ , chỉ đi chơi một ngày với 1 l...\n",
      "192  195         1  Con đường khá xa thị_trấn , chắc cỡ trên 20km ...\n",
      "193  196         1  Khách_sạn đẹp , tốt Ăn cũng khá là ngon Nhưng ...\n",
      "194  197         1  + Kết nhất cái không_gian thoáng_đãng , mát_mẻ...\n",
      "195  198         1  Nếu bạn không có nhiều thời_gian nhưng vẫn muố...\n",
      "196  199         1  Đến Ninh_Thuận vào một ngày đầu tháng 11 , từ ...\n",
      "197  200         1  Là 1 chàng trai sinh_viên còn rất trẻ với niềm...\n",
      "198  201         1  Siem_Reap với Angkor Wat quả không hổ_danh là ...\n",
      "199  202         0  Ưu_điểm nổi_bật là khuôn_viên đẹp Còn lại thì ...\n",
      "200  203         0  Cầu_Mây thuộc bản Lao_Chải - Tả_Văn , khung_cả...\n",
      "201  262         1  Nếu là để tìm_hiểu nghiên_cứu thì nơi này là n...\n",
      "\n",
      "[202 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import re\n",
    "df = pandas.read_csv('train.txt', sep='\\t')\n",
    "# print(df)\n",
    "df[\"Content\"] = df[\"Content\"].apply(remove_pattern)\n",
    "df[\"Content\"]= df[\"Content\"].apply(tokenize)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(content):\n",
    "    count_vect = CountVectorizer()\n",
    "    X_counts = count_vect.fit_transform(content)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "    return X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 2679)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "\n",
    "import pandas\n",
    "import re\n",
    "\n",
    "X_tfidf = preprocessing(df['Content'])\n",
    "print(X_tfidf.shape)\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_tfidf, df['Sentiment'], test_size=0.1, random_state=69)  \n",
    "clf = GaussianNB().fit(X_train_tfidf.toarray(), y_train_tfidf)\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 2679)\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)\n"
     ]
    }
   ],
   "source": [
    "print(X_test_tfidf.shape)\n",
    "loaded_model = pickle.load(open(\"finalized_model.sav\", 'rb'))\n",
    "print(loaded_model)\n",
    "\n",
    "y_pred = clf.predict(X_test_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47500000000000003\n",
      "0.475\n",
      "0.475\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "# average_precision = average_precision_score(test['Sentiment'], y_score)\n",
    "print(f1_score(y_test_tfidf, y_pred, average=\"macro\"))\n",
    "print(precision_score(y_test_tfidf, y_pred, average=\"macro\"))\n",
    "print(recall_score(y_test_tfidf, y_pred, average=\"macro\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1]\n",
      " [ 1 19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_test_tfidf, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9047619047619048\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.mean(y_pred == y_test_tfidf))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
